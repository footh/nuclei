-------------------------
    for mask in masks[1:]:
        mask_img = Image.open(mask)
        if mask_img.mode is not 'L': raise Exception(f"Mask image is not L mode: {mask}")
        full.paste(mask_img, mask=mask_img)
-------------------------
#     c_tconv1 = tf.keras.layers.Conv2DTranspose(TCONV_OUTPUT_CHANNELS, 
#                                               (TCONV_ROOT*2, TCONV_ROOT*2), 
#                                               strides=(TCONV_ROOT, TCONV_ROOT),
#                                               padding='same',
#                                               activation='relu',
#                                               kernel_initializer=bilinear_interp_init)(net)
#     c_tconv1_output = tf.keras.layers.Conv2D(TCONV_OUTPUT_CHANNELS, (1,1), padding='same', activation='relu')(c_tconv1)
# 
#     s_tconv1 = tf.keras.layers.Conv2DTranspose(TCONV_OUTPUT_CHANNELS, 
#                                               (TCONV_ROOT*2, TCONV_ROOT*2), 
#                                               strides=(TCONV_ROOT, TCONV_ROOT),
#                                               padding='same',
#                                               activation='relu',
#                                               kernel_initializer=bilinear_interp_init)(net)
#     s_tconv1_output = tf.keras.layers.Conv2D(TCONV_OUTPUT_CHANNELS, (1,1), padding='same', activation='relu')(s_tconv1)
# 
# 
#     c_tconv2 = tf.keras.layers.Conv2DTranspose(TCONV_OUTPUT_CHANNELS, 
#                                               (TCONV_ROOT*4, TCONV_ROOT*4), 
#                                               strides=(TCONV_ROOT*2, TCONV_ROOT*2),
#                                               padding='same',
#                                               activation='relu',
#                                               kernel_initializer=bilinear_interp_init)(net)
#     c_tconv2_output = tf.keras.layers.Conv2D(TCONV_OUTPUT_CHANNELS, (1,1), padding='same', activation='relu')(c_tconv2)
# 
#     s_tconv2 = tf.keras.layers.Conv2DTranspose(TCONV_OUTPUT_CHANNELS, 
#                                               (TCONV_ROOT*4, TCONV_ROOT*4), 
#                                               strides=(TCONV_ROOT*2, TCONV_ROOT*2),
#                                               padding='same',
#                                               activation='relu',
#                                               kernel_initializer=bilinear_interp_init)(net)
#     s_tconv2_output = tf.keras.layers.Conv2D(TCONV_OUTPUT_CHANNELS, (1,1), padding='same', activation='relu')(s_tconv2)
# 
# 
#     c_fuse = tf.add_n([c_tconv1_output, c_tconv2_output])
#     s_fuse = tf.add_n([s_tconv1_output, s_tconv2_output])
#     
#     tf.logging.debug(c_fuse)
#     tf.logging.debug(s_fuse)
-------------------------
FEATURE_ROOT = 64

def build_custom(img_input):
    upsample_convs = []
    
    # 0
    net = tf.keras.layers.Conv2D(FEATURE_ROOT, (3,3), padding='same', activation='relu')(img_input)
    
    # 1
    net = tf.keras.layers.Conv2D(FEATURE_ROOT*2, (3,3), padding='same', activation='relu')(net)
    net = tf.keras.layers.MaxPool2D(pool_size=(2,2), strides=(2,2), padding='same')(net)
    
    # 2
    net = tf.keras.layers.Conv2D(FEATURE_ROOT*4, (3,3), padding='same', activation='relu')(net)
    net = tf.keras.layers.MaxPool2D(pool_size=(2,2), strides=(2,2), padding='same')(net)

    # 3
    net = tf.keras.layers.Conv2D(FEATURE_ROOT*8, (3,3), padding='same', activation='relu')(net)
    net = tf.keras.layers.MaxPool2D(pool_size=(2,2), strides=(2,2), padding='same')(net)
    tf.logging.debug(f"net after #3: {net}")
    
    upsample_convs.append(net)
    
    # 4
    net = tf.keras.layers.Conv2D(FEATURE_ROOT*8, (3,3), padding='same', activation='relu')(net)
    net = tf.keras.layers.Dropout(rate=DROPOUT_PROB)(net)
    net = tf.keras.layers.MaxPool2D(pool_size=(2,2), strides=(2,2), padding='same')(net)
    tf.logging.debug(f"net after #4: {net}")

    upsample_convs.append(net)

    # 5
    net = tf.keras.layers.Conv2D(FEATURE_ROOT*16, (3,3), padding='same', activation='relu')(net)
    net = tf.keras.layers.Dropout(rate=DROPOUT_PROB)(net)
    net = tf.keras.layers.MaxPool2D(pool_size=(2,2), strides=(2,2), padding='same')(net)
    tf.logging.debug(f"net after #5: {net}")

    upsample_convs.append(net)

    return upsample_convs
-------------------------
- This works the same as the current TF version
def bilinear_interp_init(shape, dtype=None, partition_info=None):
    """
        Keras customer initializer for bilinear upsampling
        From: https://github.com/MarvinTeichmann/tensorflow-fcn/blob/master/fcn16_vgg.py#L245
    """
    width = shape[0]
    height = shape[1]
    f = math.ceil(width / 2.0)
    c = (2 * f - 1 - f % 2) / (2.0 * f)
    bilinear = np.zeros((width, height), dtype=dtype.as_numpy_dtype())
    for x in range(width):
        for y in range(height):
            value = (1 - abs(x / f - c)) * (1 - abs(y / f - c))
            bilinear[x,y] = value
    
    tf.logging.debug(bilinear.shape)        
    weights = np.zeros(shape, dtype=dtype.as_numpy_dtype())
    for i in range(shape[2]):
        weights[:,:,i,i] = bilinear
        
    tf.logging.debug(weights.shape)
    return weights
    #return tf.convert_to_tensor(weights, dtype=dtype, name='bilinear_interp_init')
    #return tf.constant(weights, shape=shape, dtype=dtype, name='bilinear_interp_init')
-------------------------
def temp_write(step, x, ys, yc):
    from PIL import Image
    for i in range(len(x)):
        tf.logging.info(f"x[i].shape: {x[i].shape}")
        xi = Image.fromarray(np.asarray(x[i], dtype=np.uint8))
        xi.save(f"./tmp/{step}-x-{i}.png")
        
        ysi = Image.fromarray(ys[i])
        ysi.save(f"./tmp/{step}-ys-{i}.gif")
        yci = Image.fromarray(yc[i])
        yci.save(f"./tmp/{step}-yc-{i}.gif")
-------------------------
    loss_seg = tf.nn.sigmoid_cross_entropy_with_logits(labels=labels_seg, logits=logits_seg, name='loss_seg')
    loss_con = tf.nn.sigmoid_cross_entropy_with_logits(labels=labels_con, logits=logits_con, name='loss_con')
    mean_loss_seg = tf.reduce_mean(loss_seg, name='mean_loss_seg')
    mean_loss_con = tf.reduce_mean(loss_con, name='mean_loss_con')
    total_loss = tf.add(mean_loss_seg, mean_loss_con, name='total_loss')
-------------------------
    flist = raw_file_list('test')
    flist = {os.path.basename(f) for f in flist}
    
    class_df = pd.read_csv(os.path.join('raw-data', file))
    new_class_df = pd.DataFrame(columns=['filename', 'foreground', 'background', 'set'])
    for i, row in enumerate(class_df.itertuples()):
        s = 'test' if row.filename in flist else 'train'
        new_row = [row.filename, row.foreground, row.background, s]
        new_class_df.loc[i] = new_row
    
    new_class_df.to_csv('./raw-data/classes2.csv')

loc = '/home/jfaath/Downloads/files'
class_df = pd.read_csv(os.path.join('..', 'classes.csv'))
new_class_df = pd.DataFrame(columns=['filename', 'foreground', 'background', 'rows', 'cols', 'set'])

for i, row in enumerate(class_df.itertuples()):
    name, ext = os.path.splitext(os.path.basename(row.filename))
    imga = np.asarray(Image.open(os.path.join(loc, name, 'images', row.filename)))
    new_row = [row.filename, row.foreground, row.background, imga.shape[0], imga.shape[1], row.set]
    new_class_df.loc[i] = new_row

new_class_df.to_csv('classes2.csv')
-------------------------
CONTOUR_DILATION = {
        20: 2,
        30: 2,
        40: 3,
        60: 4,
        80: 5,
        100: 6,
        150: 7,
        1000: 8
    }
-------------------------
                # Saving last epoch that is within the tolerance of the best loss
                if valid_loss - best_valid_loss < NEAR_LOSS_TOLERANCE:
                    if near_valid_loss_step is not None:
                        search_path = os.path.join(train_dir, 'best', f"{MODEL_SCOPE}_vloss-*.ckpt-{near_valid_loss_step}.*")
                        for f in gfile.Glob(search_path):
                            tf.logging.info(f"Deleting checkpoint file: {f}")
                            gfile.Remove(f)
                    near_valid_loss_step = training_step
                    checkpoint_path = os.path.join(train_dir, 'best', f"{MODEL_SCOPE}_vloss-{valid_loss:.5f}.ckpt")
                    tf.logging.info(f"Saving near loss model to {checkpoint_path}-{training_step}")
                    saver.save(sess, checkpoint_path, global_step=training_step)

-------------------------
def upsample_aft(ds_layers, img_size):
    """
        Takes in a collection of downsampled layers, applies two transposed convolutions for each input layer returns
        the results. A 1x1 convolution is added after the upsample
        
        Returns the upsampled layers for segments and contours as separate arrays
        
        kernel size calculated per here:
        http://warmspringwinds.github.io/tensorflow/tf-slim/2016/11/22/upsampling-and-image-segmentation-with-tensorflow-and-tf-slim/        

        TODO: bilinear upsampling init? This would require a conv->tconv or tconv->conv where the tconv keeps the channels
        the same and the conv adjusts to the proper channels.
        TODO: regularization? The dcan paper has L2 in the formula. What about dropout? Slim's resnet I believe has L2, need to check
    """
    
    segment_outputs = []
    contour_outputs = []
    
    for i, ds_layer in enumerate(ds_layers):
        factor = img_size // ds_layer.shape.as_list()[1]
        kernel = 2 * factor - factor % 2

        tf.logging.debug(f"layer {i+1} kernel, stride (factor): {kernel, factor}")
        tf.logging.info(f"Layer shape: {ds_layer.shape.as_list()}")

        # Default xavier_initializer is used for the weights here.
        # TODO: this is uniform, should use gaussian per dcan paper?
        net = layers.conv2d_transpose(ds_layer, 
                                      1, 
                                      kernel, 
                                      factor, 
                                      padding='SAME', 
                                      activation_fn=tf.nn.relu,
                                      scope=f"tconv{i+1}_seg")
        net = layers.conv2d(net, 1, 1, activation_fn=None, scope=f"conv{i+1}_seg")

        segment_outputs.append(net)

        net = layers.conv2d_transpose(ds_layer,
                                      1, 
                                      kernel, 
                                      factor,
                                      padding='SAME', 
                                      activation_fn=tf.nn.relu,
                                      scope=f"tconv{i+1}_con")
        net = layers.conv2d(net, 1, 1, activation_fn=None, scope=f"conv{i+1}_con")

        contour_outputs.append(net)
    
    return segment_outputs, contour_outputs

-------------------------
def upsample_old(ds_layers, img_size):
    """
        Takes in a collection of downsampled layers, applies two transposed convolutions for each input layer returns
        the results. A 1x1 convolution is performed before the transposed convolution
        
        Returns the upsampled layers for segments and contours as separate arrays
        
        kernel size calculated per here:
        http://warmspringwinds.github.io/tensorflow/tf-slim/2016/11/22/upsampling-and-image-segmentation-with-tensorflow-and-tf-slim/        

        TODO: bilinear upsampling init? This would require a conv->tconv or tconv->conv where the tconv keeps the channels
        the same and the conv adjusts to the proper channels.
    """
    
    segment_outputs = []
    contour_outputs = []
    
    for i, ds_layer in enumerate(ds_layers):
        factor = img_size // ds_layer.shape.as_list()[1]
        kernel = 2 * factor - factor % 2

        tf.logging.debug(f"layer {i+1} kernel, stride (factor): {kernel, factor}")
        tf.logging.info(f"Layer shape: {ds_layer.shape.as_list()}")

        # Default xavier_initializer is used for the weights here.
        # TODO: this is uniform, should use gaussian per dcan paper?
        net = layers.conv2d(ds_layer, ds_layer.shape.as_list()[-1], 1, activation_fn=tf.nn.relu, scope=f"conv{i+1}_seg")
        net = layers.conv2d_transpose(net, 
                                      1, 
                                      kernel, 
                                      factor, 
                                      padding='SAME', 
                                      activation_fn=None,
                                      scope=f"tconv{i+1}_seg")

        segment_outputs.append(net)

        net = layers.conv2d(ds_layer, ds_layer.shape.as_list()[-1], 1, activation_fn=tf.nn.relu, scope=f"conv{i+1}_con")
        net = layers.conv2d_transpose(net,
                                      1, 
                                      kernel, 
                                      factor,
                                      padding='SAME', 
                                      activation_fn=None,
                                      scope=f"tconv{i+1}_con")

        contour_outputs.append(net)
    
    return segment_outputs, contour_outputs


def upsample2(seg_layers, con_layers, img_size):
    """
        Takes in a collection of downsampled layers, applies two transposed convolutions for each input layer returns
        the results. A 1x1 convolution is performed before the transposed convolution
        
        Returns the upsampled layers for segments and contours as separate arrays
        
        kernel size calculated per here:
        http://warmspringwinds.github.io/tensorflow/tf-slim/2016/11/22/upsampling-and-image-segmentation-with-tensorflow-and-tf-slim/        

        TODO: bilinear upsampling init? This would require a conv->tconv or tconv->conv where the tconv keeps the channels
        the same and the conv adjusts to the proper channels.
    """
    
    segment_outputs = []
    contour_outputs = []
    
    for i in range(len(seg_layers)):
        # seg and con layers have the same shape so this is fine
        factor = img_size // seg_layers[i].shape.as_list()[1]
        kernel = 2 * factor - factor % 2

        tf.logging.debug(f"layer {i+1} kernel, stride (factor): {kernel, factor}")
        tf.logging.info(f"Layer shape: {seg_layers[i].shape.as_list()}")

        # Default xavier_initializer is used for the weights here.
        # TODO: this is uniform, should use gaussian per dcan paper?
        net = layers.conv2d(seg_layers[i], seg_layers[i].shape.as_list()[-1], 1, activation_fn=tf.nn.relu, scope=f"conv{i+1}_seg")
        net = layers.conv2d_transpose(net, 
                                      1, 
                                      kernel, 
                                      factor, 
                                      padding='SAME', 
                                      activation_fn=None,
                                      scope=f"tconv{i+1}_seg")

        segment_outputs.append(net)

        net = layers.conv2d(con_layers[i], con_layers[i].shape.as_list()[-1], 1, activation_fn=tf.nn.relu, scope=f"conv{i+1}_con")
        net = layers.conv2d_transpose(net,
                                      1, 
                                      kernel, 
                                      factor,
                                      padding='SAME', 
                                      activation_fn=None,
                                      scope=f"tconv{i+1}_con")

        contour_outputs.append(net)
    
    return segment_outputs, contour_outputs

-------------------------
    if learn_color:
        with tf.variable_scope(f"{scope}/learn_color"), slim.arg_scope([layers.conv2d],
                                                                       weights_regularizer=slim.l2_regularizer(l2_weight_decay)):
            input = layers.conv2d(input, 10, 1, padding='VALID', scope=f"conv1")
            input = layers.conv2d(input, 3, 1, padding='VALID', scope=f"conv2")

-------------------------
def test():
    data_processor = data.DataProcessor(src='vtest', img_size=256, testing_pct=100)

    sample_tiles, sample_info = data_processor.batch_test(offset=0, overlap_const=2)

    tile_rows, tile_cols = sample_tiles.shape[0:2]
    tf.logging.info(f"tile_rows, tile_cols: {tile_rows}, {tile_cols}")

    sample_batch = sample_tiles.reshape(tile_rows * tile_cols, *sample_tiles.shape[2:])

    for flip in range(2):
        for rotation in range(4):
            sample_batch = affine_augment(sample_batch, flip, rotation)
            for i, tile in enumerate(sample_batch):
                print(f"max, min before: {np.max(tile)}, {np.min(tile)}")
                tile += data.VGG_RGB_MEANS
                print(f"max, min after: {np.max(tile)}, {np.min(tile)}")
                Image.fromarray(np.asarray(tile, dtype=np.uint8)).save(f"/tmp/nuclei/data/tile-pre{i}-f{flip}-r{rotation}.png")
                tile -= data.VGG_RGB_MEANS

            sample_batch = affine_augment(sample_batch, flip, -rotation)
            for i, tile in enumerate(sample_batch):
                print(f"max, min before: {np.max(tile)}, {np.min(tile)}")
                tile += data.VGG_RGB_MEANS
                print(f"max, min after: {np.max(tile)}, {np.min(tile)}")
                Image.fromarray(np.asarray(tile, dtype=np.uint8)).save(f"/tmp/nuclei/data/tile-post{i}-f{flip}-r{rotation}.png")
                tile -= data.VGG_RGB_MEANS
-------------------------
    def batch_test_abut(self, offset=0, normalize=True):
        """
            Same as 'batch_test' except tiles are not overlapped but are abut
        """
        sample_info = {}

        sample_id = self.data_index['test'][offset]
        sample_info['id'] = sample_id

        sample_src = np.asarray(Image.open(os.path.join(self.src_dir, f"{sample_id}-{IMG_SRC}.{IMG_EXT}")))
        if _DEBUG_WRITE_:
            img = Image.fromarray(sample_src)
            img.save(f"/tmp/nuclei/original.png")
        tf.logging.debug(f"sample original shape: {sample_src.shape}")
        sample_info['orig_shape'] = sample_src.shape
        sample_src = sample_src[:, :, 0:IMG_CHANNELS]

        padding_row = (0, 0)
        if sample_src.shape[0] % self.img_size > 0:
            padding_ttl = self.img_size - (sample_src.shape[0] % self.img_size)
            padding_row = (padding_ttl // 2, padding_ttl - padding_ttl // 2)

        padding_col = (0, 0)
        if sample_src.shape[1] % self.img_size > 0:
            padding_ttl = self.img_size - (sample_src.shape[1] % self.img_size)
            padding_col = (padding_ttl // 2, padding_ttl - padding_ttl // 2)

        tf.logging.debug(f"padding_row, padding_col: {padding_row}, {padding_col}")
        sample_info['padding_row'] = padding_row
        sample_info['padding_col'] = padding_col
        sample_src = np.pad(sample_src, (padding_row, padding_col, (0, 0)), mode='reflect')

        tf.logging.debug(f"sample padded shape: {sample_src.shape}")
        prows, pcols, _ = sample_src.shape

        sample_src = np.asarray(sample_src, dtype=np.float32)
        if normalize:
            # Slim's vgg_preprocessing only does the mean subtraction (not the RGB to BGR)
            sample_src = sample_src - VGG_RGB_MEANS

        tiles = []
        for i in range(0, prows, self.img_size):
            tiles.append([])
            for j in range(0, pcols, self.img_size):
                tile = sample_src[i:i + self.img_size, j:j + self.img_size, :]
                tiles[-1].append(tile)

        tiles = np.asarray(tiles)
        tf.logging.debug(f"tiles.shape: {tiles.shape}")

        return tiles, sample_info

-------------------------

def evaluate_abut(trained_checkpoint, src='test', pixel_threshold=0.5, contour_threshold=0.5):
    # TODO: parameterize
    window_size = train.IMG_SIZE

    sess = tf.InteractiveSession()

    with tf.variable_scope(f"{train.MODEL_SCOPE}/data"):
        img_input = tf.placeholder(tf.float32, [None, window_size, window_size, 3], name='img_input')

    pred_full = build_model(img_input)

    train.restore_from_checkpoint(trained_checkpoint, sess)

    data_processor = data.DataProcessor(src=src, img_size=window_size, testing_pct=100)

    for cnt in range(5, 10):
        sample_tiles, sample_info = data_processor.batch_test_abut(offset=cnt)

        # Prediction --------------------------------
        tile_rows, tile_cols = sample_tiles.shape[0:2]
        tf.logging.info(f"tile_rows, tile_cols: {tile_rows}, {tile_cols}")

        # TODO: with large number of tiles, may need to batch this further
        sample_batch = sample_tiles.reshape(tile_rows * tile_cols, *sample_tiles.shape[2:])

        sample_pred = sess.run(pred_full, feed_dict={img_input: sample_batch})
        tf.logging.info(f"sample_pred.shape: {sample_pred.shape}")

        sample_pred = sample_pred.reshape(tile_rows, tile_cols, *sample_pred.shape[1:])
        # -------------------------------------------

        full_pred_rows = tile_rows * window_size
        full_pred_cols = tile_cols * window_size
        tf.logging.info(f"full_pred_rows, full_pred_cols: {full_pred_rows}, {full_pred_cols}")

        result_seg = np.zeros((full_pred_rows, full_pred_cols), dtype=np.float32)
        result_con = np.zeros((full_pred_rows, full_pred_cols), dtype=np.float32)
        divisors = np.zeros((full_pred_rows, full_pred_cols), dtype=np.float32)

        for i, row_start in enumerate(range(0, full_pred_rows, window_size)):
            for j, col_start in enumerate(range(0, full_pred_cols, window_size)):
                seg = sample_pred[i, j, :, :, 0]
                con = sample_pred[i, j, :, :, 1]
                result_seg[row_start:row_start + window_size, col_start:col_start + window_size] += seg
                result_con[row_start:row_start + window_size, col_start:col_start + window_size] += con
                divisors[row_start:row_start + window_size, col_start:col_start + window_size] += 1.

        padding_row = sample_info['padding_row']
        padding_col = sample_info['padding_col']
        result_seg = result_seg[padding_row[0]:full_pred_rows - padding_row[1], padding_col[0]:full_pred_cols - padding_col[1]]
        result_con = result_con[padding_row[0]:full_pred_rows - padding_row[1], padding_col[0]:full_pred_cols - padding_col[1]]

        # util._debug_output(data_processor, sample_info['id'], result_seg, result_con, divisors)

        result = post_process(result_seg, result_con)

        for rle_label in rle_labels(result, dilate=None):
            rle_results.append([sample_info['id']] + [rle_label])

    return rle_results
-------------------------
def kmeans(img_size=(256, 256), clusters=3):
    """
        Kmeans testing
    """
    print(f"Running kmeans...")
    imgs = as_images(size=img_size)

    vec_size = img_size[0]*img_size[1]
    x = np.zeros((len(imgs), vec_size), dtype=np.float32)
    for i, img in enumerate(imgs):
        imga = np.asarray(img)
        imga = imga[:,:,0:3]
        imga = np.mean(imga, axis=2)
        x[i] = imga.reshape(vec_size)

    print(f"Data processed...")

    km = KMeans(n_clusters=clusters, random_state=0).fit(x)
    for i in range(clusters):
        lbl_count = np.sum(km.labels_ == i)
        print(f"cluster {i}: {lbl_count}")
        print(f"cluster {i} ratio: {lbl_count/len(imgs)}")

    return x, km


-------------------------
-------------------------
-------------------------
-------------------------

