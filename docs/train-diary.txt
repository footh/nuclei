TRAINING NOTES:
(For these notes, each run will only detail changes from previous runs)

First attempt: [5000, 3000, 3000], [0.001, 0.0005, 0.0001]. One pixel contours. Default init
Converged to ~0.13 after a few thousand steps. Discarded

Used resnet init:
Converged to 0.12611 after 1200 steps then val loss just got worse. Conclusion: definitely start with resnet, but overfit very quickly.

Used thicker contours and rotation and flip data aug: 
Took much longer to converge, hit a val loss of 0.14941 on the last step (11,000) which shows signs that it probably could benefit from more training. BUT the val
loss at step 2700 was 0.15522. So lots of steps with little change.

Keeping resnet init, thicker contours, basic data aug
[5000,5000,5000] (same learning rates), Adam epsilon 0.1 (vs 1e-8):
Learning slowed a ton, 0.18200 on step 9900, let it run for a couple thousand more (at the really low learning rate) and there was no improvement so killed it

[10000, 5000, 5000], [0.001, 0.0007, 0.0003], Adam epsilon 0.1:
Similar to above run but extending the steps and increasing the learning rates to see if it would move towards the pre-epsilon change results. Achieved 0.16770 on
step 18300. The five best were at steps 12000 (at 0.17468), 12600, 13500, 15600 and 18300. So not sure if even more steps with higher rate lingering longer would work? 
The training losses vary greatly (obviously with batch size 5), but the eyeball test seem to show a pretty close correlation to val loss. Could this mean less 
overfitting? Unsure what to do with the epsilon experiment. 

[15000, 15000, 15000], [0.001, 0.0007, 0.0003], Adam epsilon 0.1, consistent validation data, per-label weighting:
tensorboard --logdir=/home/jfaath/dev/projects/nuclei/training-runs/bs5_resinit3/summary/train
Loss scores are significantly lower relatively due to the label weighting so it's hard to make a comparison with previous runs. Looks like loss peaked
at around 15K steps and remained steady after though the 'best' 5 checkpoints were all near the very end, including the last one (step 45K) as the best.
The validation loss is very similar to the training loss which may be good.

[15000, 15000, 15000], [0.001, 0.0007, 0.0003], Adam epsilon 0.1, consistent validation data, per-label weighting, THIN contours again:
tensorboard --logdir=/home/jfaath/dev/projects/nuclei/training-runs/bs5_resinit4/summary/train
Cut at around 17K iterations. The contour predictions were even worse in terms of thickness. So I conclude that thicker contours provide more ground truth
for the algorithm to key off of. I *think* the segments weren't as good with the smaller training steps but not entirely sure.

[10000, 5000, 5000], [0.001, 0.0007, 0.0003], Adam epsilon 1e-08, THICK contours:
tensorboard --logdir=training-runs/bs5_resinit5/summary/train

2/25:
[10000, 5000, 5000], [0.001, 0.0007, 0.0003], Adam epsilon 1e-08, variable contours, edge guarantee, type-distributed val set
Used perimeter based contour widths. No weights on loss (proportions were basically even)
tensorboard --logdir=training-runs/bs5_resinit6/summary/train
IOUs steadily went up, loss barely moved. 

2/26
[8000, 8000, 8000, 8000], [0.002, 0.001, 0.0007, 0.0003], Adam epsilon 1e-08, variable contours, edge guarantee, type-distributed val set
Used minor axis based contour lengths. No weights on loss (proportions favored segments like .13 to .09)
tensorboard --logdir=training-runs/bs5_resinit7/summary/train
Validation loss stopped going down at 15600. 

54000 steps, momentum optimizer, .001 base LR, 0.8 decay base, decays after 10 val loss with no progress, 0.2 momentum
Used minor axis based contour lengths. No weights on loss (proportions favored segments like .13 to .09)
tensorboard --logdir=training-runs/bs5_resinit8/summary/train
Validation loss was still going down even at 54000, did not reach the val loss of Adam run above. 

2/27:
54000 steps, momentum optimizer, .001 base LR, 0.8 decay base, decays after 10 val loss with no progress, 0.8 momentum
Used minor axis based contour lengths. No weights on loss (proportions favored segments like .13 to .09)
tensorboard --logdir=training-runs/bs5_resinit9/summary/train

32000 steps, Adam 1e-4 epsilon, 0.001 base LR, 0.8 decay base, decay after 15...oops
Minor axis based contour lengths, No weights on loss
Added 1x1x1 convolution layer after tconv :shrug:
tensorboard --logdir=training-runs/bs5_resinit10/summary/train
- At step 155, the IOU contour went from 0.000 to ~0.400, higher than the segment at the time. Weird.
- Killed after 22K steps, val loss was going up for a while