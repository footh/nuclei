TRAINING NOTES:
(For these notes, each run will only detail changes from previous runs)

First attempt: [5000, 3000, 3000], [0.001, 0.0005, 0.0001]. One pixel contours. Default init
Converged to ~0.13 after a few thousand steps. Discarded

Used resnet init:
Converged to 0.12611 after 1200 steps then val loss just got worse. Conclusion: definitely start with resnet, but overfit very quickly.

Used thicker contours and rotation and flip data aug: 
Took much longer to converge, hit a val loss of 0.14941 on the last step (11,000) which shows signs that it probably could benefit from more training. BUT the val
loss at step 2700 was 0.15522. So lots of steps with little change.

Keeping resnet init, thicker contours, basic data aug
[5000,5000,5000] (same learning rates), Adam epsilon 0.1 (vs 1e-8):
Learning slowed a ton, 0.18200 on step 9900, let it run for a couple thousand more (at the really low learning rate) and there was no improvement so killed it

[10000, 5000, 5000], [0.001, 0.0007, 0.0003], Adam epsilon 0.1:
Similar to above run but extending the steps and increasing the learning rates to see if it would move towards the pre-epsilon change results. Achieved 0.16770 on
step 18300. The five best were at steps 12000 (at 0.17468), 12600, 13500, 15600 and 18300. So not sure if even more steps with higher rate lingering longer would work? 
The training losses vary greatly (obviously with batch size 5), but the eyeball test seem to show a pretty close correlation to val loss. Could this mean less 
overfitting? Unsure what to do with the epsilon experiment. 

[15000, 15000, 15000], [0.001, 0.0007, 0.0003], Adam epsilon 0.1, consistent validation data, per-label weighting:
tensorboard --logdir=/home/jfaath/dev/projects/nuclei/training-runs/bs5_resinit3/summary/train
Loss scores are significantly lower relatively due to the label weighting so it's hard to make a comparison with previous runs. Looks like loss peaked
at around 15K steps and remained steady after though the 'best' 5 checkpoints were all near the very end, including the last one (step 45K) as the best.
The validation loss is very similar to the training loss which may be good.

[15000, 15000, 15000], [0.001, 0.0007, 0.0003], Adam epsilon 0.1, consistent validation data, per-label weighting, THIN contours again:
tensorboard --logdir=/home/jfaath/dev/projects/nuclei/training-runs/bs5_resinit4/summary/train
Cut at around 17K iterations. The contour predictions were even worse in terms of thickness. So I conclude that thicker contours provide more ground truth
for the algorithm to key off of. I *think* the segments weren't as good with the smaller training steps but not entirely sure.

[10000, 5000, 5000], [0.001, 0.0007, 0.0003], Adam epsilon 1e-08, THICK contours:


2/25:
[10000, 5000, 5000], [0.001, 0.0007, 0.0003], Adam epsilon 1e-08, variable contours, edge guarantee, type-distributed val set
tensorboard --logdir=training-runs/bs5_resinit6/summary/train
IOUs steadily went up, loss barely moved. Used perimeter based contour widths

2/26
[8000, 8000, 8000, 8000], [0.005, 0.001, 0.0007, 0.0003], Adam epsilon 1e-08, variable contours, edge guarantee, type-distributed val set
tensorboard --logdir=training-runs/bs5_resinit7/summary/train